{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%import tensorflow as tf\n"
    }
   },
   "outputs": [],
   "source": [
    "import tensorflow as tf\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "import config\n",
    "\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from DataLoader import FeatureDictionary, DataParser\n",
    "\n",
    "from DCN import DCN\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "pycharm": {
     "is_executing": false,
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>id</th>\n",
       "      <th>target</th>\n",
       "      <th>ps_ind_01</th>\n",
       "      <th>ps_ind_02_cat</th>\n",
       "      <th>ps_ind_03</th>\n",
       "      <th>ps_ind_04_cat</th>\n",
       "      <th>ps_ind_05_cat</th>\n",
       "      <th>ps_ind_06_bin</th>\n",
       "      <th>ps_ind_07_bin</th>\n",
       "      <th>ps_ind_08_bin</th>\n",
       "      <th>...</th>\n",
       "      <th>ps_calc_11</th>\n",
       "      <th>ps_calc_12</th>\n",
       "      <th>ps_calc_13</th>\n",
       "      <th>ps_calc_14</th>\n",
       "      <th>ps_calc_15_bin</th>\n",
       "      <th>ps_calc_16_bin</th>\n",
       "      <th>ps_calc_17_bin</th>\n",
       "      <th>ps_calc_18_bin</th>\n",
       "      <th>ps_calc_19_bin</th>\n",
       "      <th>ps_calc_20_bin</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>5</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>5</td>\n",
       "      <td>8</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>13</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>...</td>\n",
       "      <td>4</td>\n",
       "      <td>2</td>\n",
       "      <td>7</td>\n",
       "      <td>7</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>16</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>2</td>\n",
       "      <td>2</td>\n",
       "      <td>4</td>\n",
       "      <td>9</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>17</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>...</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>3</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 59 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   id  target  ps_ind_01  ps_ind_02_cat  ps_ind_03  ps_ind_04_cat  \\\n",
       "0   7       0          2              2          5              1   \n",
       "1   9       0          1              1          7              0   \n",
       "2  13       0          5              4          9              1   \n",
       "3  16       0          0              1          2              0   \n",
       "4  17       0          0              2          0              1   \n",
       "\n",
       "   ps_ind_05_cat  ps_ind_06_bin  ps_ind_07_bin  ps_ind_08_bin  ...  \\\n",
       "0              0              0              1              0  ...   \n",
       "1              0              0              0              1  ...   \n",
       "2              0              0              0              1  ...   \n",
       "3              0              1              0              0  ...   \n",
       "4              0              1              0              0  ...   \n",
       "\n",
       "   ps_calc_11  ps_calc_12  ps_calc_13  ps_calc_14  ps_calc_15_bin  \\\n",
       "0           9           1           5           8               0   \n",
       "1           3           1           1           9               0   \n",
       "2           4           2           7           7               0   \n",
       "3           2           2           4           9               0   \n",
       "4           3           1           1           3               0   \n",
       "\n",
       "   ps_calc_16_bin  ps_calc_17_bin  ps_calc_18_bin  ps_calc_19_bin  \\\n",
       "0               1               1               0               0   \n",
       "1               1               1               0               1   \n",
       "2               1               1               0               1   \n",
       "3               0               0               0               0   \n",
       "4               0               0               1               1   \n",
       "\n",
       "   ps_calc_20_bin  \n",
       "0               1  \n",
       "1               0  \n",
       "2               0  \n",
       "3               0  \n",
       "4               0  \n",
       "\n",
       "[5 rows x 59 columns]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain = pd.read_csv(config.TRAIN_FILE)\n",
    "dfTest = pd.read_csv(config.TEST_FILE)\n",
    "dfTrain.head()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "invalid character in identifier (<ipython-input-36-19f9e8120f22>, line 2)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  File \u001b[0;32m\"<ipython-input-36-19f9e8120f22>\"\u001b[0;36m, line \u001b[0;32m2\u001b[0m\n\u001b[0;31m    “““\u001b[0m\n\u001b[0m      ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid character in identifier\n"
     ]
    }
   ],
   "source": [
    "def preprocess(df):\n",
    "    # 将除了 id target 以外的每一列进行处理\n",
    "    # 统计该样本所有特征中值==-1的个数，新建一个missing_feat 特征进行保存\n",
    "    # 新建一个ps_car_13_x_ps_reg_03特征，其值为  ps_car_13*ps_reg_03\n",
    "    cols = [c for c in df.columns if c not in [\"id\", \"target\"]] \n",
    "    \n",
    "    df[\"missing_feat\"] = np.sum((df[cols] == -1).values, axis=1) # .values 查看具体的数据值 将DataFrame转换为ndarray,\n",
    "    df[\"ps_car_13_x_ps_reg_03\"] = df[\"ps_car_13\"] * df[\"ps_reg_03\"]\n",
    "    return df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 61), (2000, 60))"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dfTrain = preprocess(dfTrain)\n",
    "dfTest = preprocess(dfTest)\n",
    "dfTrain.shape,dfTest.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 排除 id target 之外我们设定忽略的特征\n",
    "cols = [c for c in dfTrain.columns if c not in [\"id\", \"target\"]]\n",
    "cols = [c for c in cols if (not c in config.IGNORE_COLS)] "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "((10000, 39), (10000,), (2000, 39), (2000,))"
      ]
     },
     "execution_count": 41,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 产生训练及测试数据\n",
    "X_train = dfTrain[cols].values\n",
    "y_train = dfTrain[\"target\"].values\n",
    "X_test = dfTest[cols].values\n",
    "ids_test = dfTest[\"id\"].values\n",
    "X_train.shape,y_train.shape,X_test.shape,ids_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(array([   0,    1,    4, ..., 9996, 9997, 9999]),\n",
       "  array([   2,    3,    8, ..., 9991, 9994, 9998])),\n",
       " (array([   0,    1,    2, ..., 9996, 9997, 9998]),\n",
       "  array([   4,    5,    7, ..., 9992, 9993, 9999])),\n",
       " (array([   2,    3,    4, ..., 9994, 9998, 9999]),\n",
       "  array([   0,    1,    6, ..., 9995, 9996, 9997]))]"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#StratifiedKFold函数采用分层划分的方法（分层随机抽样思想），验证集中不同类别占比与原始样本的比例保持一致，故StratifiedKFold在做划分的时候需要传入标签特征。\n",
    "# 注意这里folds里面存储的知识样本的索引\n",
    "folds = list(StratifiedKFold(n_splits=config.NUM_SPLITS, shuffle=True,\n",
    "                             random_state=config.RANDOM_SEED).split(X_train, y_train))\n",
    "folds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcn_params = {\n",
    "\n",
    "    \"embedding_size\": 8,\n",
    "    \"deep_layers\": [32, 32],\n",
    "    \"dropout_deep\": [0.5, 0.5, 0.5],\n",
    "    \"deep_layers_activation\": tf.nn.relu,\n",
    "    \"epoch\": 30,\n",
    "    \"batch_size\": 1024,\n",
    "    \"learning_rate\": 0.001,\n",
    "    \"optimizer_type\": \"adam\",\n",
    "    \"batch_norm\": 1,\n",
    "    \"batch_norm_decay\": 0.995,\n",
    "    \"l2_reg\": 0.01,\n",
    "    \"verbose\": True,\n",
    "    \"random_seed\": config.RANDOM_SEED,\n",
    "    \"cross_layer_num\":3\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "G:\\tf_learn\\tensorflow_practice-master\\recommendation\\Basic-DCN-Demo\\DataLoader.py:21: FutureWarning: Sorting because non-concatenation axis is not aligned. A future version\n",
      "of pandas will change to not sort by default.\n",
      "\n",
      "To accept the future behavior, pass 'sort=False'.\n",
      "\n",
      "To retain the current behavior and silence the warning, pass 'sort=True'.\n",
      "\n",
      "  df = pd.concat([self.trainfile,self.testfile])\n"
     ]
    }
   ],
   "source": [
    "# 采用更方便的方式得到特征的数量和特征字典，与上一篇不同的是忽略了NUMERIC_COLS里面的特征\n",
    "fd = FeatureDictionary(dfTrain,dfTest,numeric_cols=config.NUMERIC_COLS,\n",
    "                    ignore_cols=config.IGNORE_COLS,\n",
    "                        cate_cols = config.CATEGORICAL_COLS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "247\n",
      "{'ps_car_01_cat': {10: 0, 11: 1, 7: 2, 6: 3, 9: 4, 5: 5, 4: 6, 8: 7, 3: 8, 0: 9, 2: 10, 1: 11, -1: 12}, 'ps_car_02_cat': {1: 13, 0: 14}, 'ps_car_03_cat': {-1: 15, 0: 16, 1: 17}, 'ps_car_04_cat': {0: 18, 1: 19, 8: 20, 9: 21, 2: 22, 6: 23, 3: 24, 7: 25, 4: 26, 5: 27}, 'ps_car_05_cat': {1: 28, -1: 29, 0: 30}, 'ps_car_06_cat': {4: 31, 11: 32, 14: 33, 13: 34, 6: 35, 15: 36, 3: 37, 0: 38, 1: 39, 10: 40, 12: 41, 9: 42, 17: 43, 7: 44, 8: 45, 5: 46, 2: 47, 16: 48}, 'ps_car_07_cat': {1: 49, -1: 50, 0: 51}, 'ps_car_08_cat': {0: 52, 1: 53}, 'ps_car_09_cat': {0: 54, 2: 55, 3: 56, 1: 57, -1: 58, 4: 59}, 'ps_car_10_cat': {1: 60, 0: 61, 2: 62}, 'ps_car_11': {2: 63, 3: 64, 1: 65, 0: 66}, 'ps_car_11_cat': {12: 67, 19: 68, 60: 69, 104: 70, 82: 71, 99: 72, 30: 73, 68: 74, 20: 75, 36: 76, 101: 77, 103: 78, 41: 79, 59: 80, 43: 81, 64: 82, 29: 83, 95: 84, 24: 85, 5: 86, 28: 87, 87: 88, 66: 89, 10: 90, 26: 91, 54: 92, 32: 93, 38: 94, 83: 95, 89: 96, 49: 97, 93: 98, 1: 99, 22: 100, 85: 101, 78: 102, 31: 103, 34: 104, 7: 105, 8: 106, 3: 107, 46: 108, 27: 109, 25: 110, 61: 111, 16: 112, 69: 113, 40: 114, 76: 115, 39: 116, 88: 117, 42: 118, 75: 119, 91: 120, 23: 121, 2: 122, 71: 123, 90: 124, 80: 125, 44: 126, 92: 127, 72: 128, 96: 129, 86: 130, 62: 131, 33: 132, 67: 133, 73: 134, 77: 135, 18: 136, 21: 137, 74: 138, 37: 139, 48: 140, 70: 141, 13: 142, 15: 143, 102: 144, 53: 145, 65: 146, 100: 147, 51: 148, 79: 149, 52: 150, 63: 151, 94: 152, 6: 153, 57: 154, 35: 155, 98: 156, 56: 157, 97: 158, 55: 159, 84: 160, 50: 161, 4: 162, 58: 163, 9: 164, 17: 165, 11: 166, 45: 167, 14: 168, 81: 169, 47: 170}, 'ps_ind_01': {2: 171, 1: 172, 5: 173, 0: 174, 4: 175, 3: 176, 6: 177, 7: 178}, 'ps_ind_02_cat': {2: 179, 1: 180, 4: 181, 3: 182, -1: 183}, 'ps_ind_03': {5: 184, 7: 185, 9: 186, 2: 187, 0: 188, 4: 189, 3: 190, 1: 191, 11: 192, 6: 193, 8: 194, 10: 195}, 'ps_ind_04_cat': {1: 196, 0: 197, -1: 198}, 'ps_ind_05_cat': {0: 199, 1: 200, 4: 201, 3: 202, 6: 203, 5: 204, -1: 205, 2: 206}, 'ps_ind_06_bin': {0: 207, 1: 208}, 'ps_ind_07_bin': {1: 209, 0: 210}, 'ps_ind_08_bin': {0: 211, 1: 212}, 'ps_ind_09_bin': {0: 213, 1: 214}, 'ps_ind_10_bin': {0: 215, 1: 216}, 'ps_ind_11_bin': {0: 217, 1: 218}, 'ps_ind_12_bin': {0: 219, 1: 220}, 'ps_ind_13_bin': {0: 221, 1: 222}, 'ps_ind_14': {0: 223, 1: 224, 2: 225, 3: 226}, 'ps_ind_15': {11: 227, 3: 228, 12: 229, 8: 230, 9: 231, 6: 232, 13: 233, 4: 234, 10: 235, 5: 236, 7: 237, 2: 238, 0: 239, 1: 240}, 'ps_ind_16_bin': {0: 241, 1: 242}, 'ps_ind_17_bin': {1: 243, 0: 244}, 'ps_ind_18_bin': {0: 245, 1: 246}}\n"
     ]
    }
   ],
   "source": [
    "print(fd.feat_dim)\n",
    "print(fd.feat_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 对特征进行转换，\n",
    "data_parser = DataParser(feat_dict=fd)\n",
    "cate_Xi_train, cate_Xv_train, numeric_Xv_train,y_train = data_parser.parse(df=dfTrain, has_label=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(30, 30, 10000, 10000)"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cate_Xi_train[0]),len(cate_Xv_train[0]),len(numeric_Xv_train),len(y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [],
   "source": [
    "dcn_params[\"cate_feature_size\"] = fd.feat_dim # 247\n",
    "dcn_params[\"field_size\"] = len(cate_Xi_train[0]) # 30\n",
    "dcn_params['numeric_feature_size'] = len(config.NUMERIC_COLS) # 9"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "6666 30\n",
      "6666 30\n",
      "6666 9\n",
      "10000\n",
      "WARNING:tensorflow:From G:\\tf_learn\\tensorflow_practice-master\\recommendation\\Basic-DCN-Demo\\DCN.py:54: The name tf.set_random_seed is deprecated. Please use tf.compat.v1.set_random_seed instead.\n",
      "\n",
      "WARNING:tensorflow:From G:\\tf_learn\\tensorflow_practice-master\\recommendation\\Basic-DCN-Demo\\DCN.py:56: The name tf.placeholder is deprecated. Please use tf.compat.v1.placeholder instead.\n",
      "\n",
      "WARNING:tensorflow:From G:\\tf_learn\\tensorflow_practice-master\\recommendation\\Basic-DCN-Demo\\DCN.py:164: The name tf.random_normal is deprecated. Please use tf.random.normal instead.\n",
      "\n",
      "WARNING:tensorflow:From G:\\tf_learn\\tensorflow_practice-master\\recommendation\\Basic-DCN-Demo\\DCN.py:84: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "WARNING:tensorflow:From G:\\tf_learn\\tensorflow_practice-master\\recommendation\\Basic-DCN-Demo\\DCN.py:110: The name tf.losses.log_loss is deprecated. Please use tf.compat.v1.losses.log_loss instead.\n",
      "\n",
      "WARNING:tensorflow:From g:\\python\\lib\\site-packages\\tensorflow\\python\\ops\\losses\\losses_impl.py:121: add_dispatch_support.<locals>.wrapper (from tensorflow.python.ops.array_ops) is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Use tf.where in 2.0, which has the same broadcast rule as np.where\n",
      "WARNING:tensorflow:\n",
      "The TensorFlow contrib module will not be included in TensorFlow 2.0.\n",
      "For more information, please see:\n",
      "  * https://github.com/tensorflow/community/blob/master/rfcs/20180907-contrib-sunset.md\n",
      "  * https://github.com/tensorflow/addons\n",
      "  * https://github.com/tensorflow/io (for I/O related ops)\n",
      "If you depend on functionality not listed there, please file an issue.\n",
      "\n",
      "WARNING:tensorflow:From G:\\tf_learn\\tensorflow_practice-master\\recommendation\\Basic-DCN-Demo\\DCN.py:126: The name tf.train.AdamOptimizer is deprecated. Please use tf.compat.v1.train.AdamOptimizer instead.\n",
      "\n",
      "WARNING:tensorflow:From G:\\tf_learn\\tensorflow_practice-master\\recommendation\\Basic-DCN-Demo\\DCN.py:139: The name tf.train.Saver is deprecated. Please use tf.compat.v1.train.Saver instead.\n",
      "\n",
      "WARNING:tensorflow:From G:\\tf_learn\\tensorflow_practice-master\\recommendation\\Basic-DCN-Demo\\DCN.py:140: The name tf.global_variables_initializer is deprecated. Please use tf.compat.v1.global_variables_initializer instead.\n",
      "\n",
      "WARNING:tensorflow:From G:\\tf_learn\\tensorflow_practice-master\\recommendation\\Basic-DCN-Demo\\DCN.py:141: The name tf.Session is deprecated. Please use tf.compat.v1.Session instead.\n",
      "\n",
      "#params: 13055\n"
     ]
    }
   ],
   "source": [
    "_get = lambda x, l: [x[i] for i in l]\n",
    "\n",
    "for i, (train_idx, valid_idx) in enumerate(folds):\n",
    "    cate_Xi_train_, cate_Xv_train_, numeric_Xv_train_,y_train_ = _get(cate_Xi_train, train_idx), _get(cate_Xv_train, train_idx),_get(numeric_Xv_train, train_idx), _get(y_train, train_idx)\n",
    "    cate_Xi_valid_, cate_Xv_valid_, numeric_Xv_valid_,y_valid_ = _get(cate_Xi_train, valid_idx), _get(cate_Xv_train, valid_idx),_get(numeric_Xv_train, valid_idx), _get(y_train, valid_idx)\n",
    "    print(len(cate_Xi_train_),len(cate_Xi_train_[0])) \n",
    "    print(len(cate_Xv_train_),len(cate_Xv_train_[0]))    \n",
    "    print(len(numeric_Xv_train_),len(numeric_Xv_train_[0]))\n",
    "    print(len(y_train))\n",
    "\n",
    "        \n",
    "    \n",
    "    dcn =  DCN(**dcn_params)\n",
    "    break\n",
    "    dcn.fit(cate_Xi_train_, cate_Xv_train_, numeric_Xv_train_,y_train_, cate_Xi_valid_, cate_Xv_valid_, numeric_Xv_valid_,y_valid_)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DCN(batch_norm=1, batch_norm_decay=0.995, batch_size=1024,\n",
       "    cate_feature_size=247, cross_layer_num=3, deep_layers=[32, 32],\n",
       "    deep_layers_activation=<function relu at 0x000001F52A3076A8>,\n",
       "    dropout_deep=None, embedding_size=8, epoch=30,\n",
       "    eval_metric=<function roc_auc_score at 0x000001F528B10B70>, field_size=30,\n",
       "    greater_is_better=True, l2_reg=0.01, learning_rate=0.001,\n",
       "    loss_type='logloss', numeric_feature_size=9, optimizer_type='adam',\n",
       "    random_seed=2017, verbose=True)"
      ]
     },
     "execution_count": 91,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dcn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 调用DCN模型后，会调用类里面的self._init_graph()进行初始化。self._init_graph\n",
    "\n",
    "def _init_graph(self):\n",
    "    self.graph = tf.Graph()\n",
    "    with self.graph.as_default():\n",
    "        tf.set_random_seed(self.random_seed)\n",
    "            self.feat_index = tf.placeholder(tf.int32,shape=[None,None],name='feat_index') #【B,30】\n",
    "            self.feat_value = tf.placeholder(tf.float32,shape=[None,None],name='feat_value')#[B,30]\n",
    "            self.numeric_value = tf.placeholder(tf.float32,[None,None],name='num_value')#[B,9]\n",
    "            self.label = tf.placeholder(tf.float32,shape=[None,1],name='label')#[B,1]\n",
    "            self.dropout_keep_deep = tf.placeholder(tf.float32,shape=[None],name='dropout_deep_deep')\n",
    "            self.train_phase = tf.placeholder(tf.bool,name='train_phase')\n",
    "            self.weights = self._initialize_weights()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#首先调用了self._initialize_weights()对权重参数进行初始化\n",
    "# 根据论文中的结构图，从下倒上进行模型的搭建 首先是embedding层，\n",
    "# 和之前一样，创建参数w和bias的tensor\n",
    "# 这里self.cate_feature_size为 247，还记得在DeepFM embedding层特征维度是多少嘛？254，这是因为这次少了7个连续特征\n",
    "#除了对于特征的处理上，DeepFM将连续特征离散化作为整体输入，而DCN将连续特征剥离出来不经过embedding，与embedding后的离散特征拼接\n",
    "weights['feature_embeddings'] = tf.Variable(tf.random_normal([self.cate_feature_size,self.embedding_size],0.0,0.01),\n",
    "            name='feature_embeddings') \n",
    "weights['feature_bias'] = tf.Variable(tf.random_normal([self.cate_feature_size,1],0.0,1.0),name='feature_bias')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#deep layers \n",
    "# 这部分和DeepFm模型差不多，self.total_size = self.field_size * self.embedding_size + self.numeric_feature_size = 30*8+9 = 249\n",
    "# 这里也可以看出 DCN将连续特征剥离出来不经过embedding，与embedding后的离散特征拼接\n",
    "num_layer = len(self.deep_layers)\n",
    "glorot = np.sqrt(2.0/(self.total_size + self.deep_layers[0]))\n",
    "weights['deep_layer_0'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(self.total_size,self.deep_layers[0])),dtype=np.float32)\n",
    "weights['deep_bias_0'] = tf.Variable( np.random.normal(loc=0,scale=glorot,size=(1,self.deep_layers[0])),dtype=np.float32)\n",
    "\n",
    "#后续层 同样为了方便 我们只搭建两层【32,32】\n",
    "for i in range(1,num_layer):\n",
    "    glorot = np.sqrt(2.0 / (self.deep_layers[i - 1] + self.deep_layers[i]))\n",
    "    weights[\"deep_layer_%d\" % i] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(self.deep_layers[i - 1], self.deep_layers[i])),\n",
    "                dtype=np.float32)  # layers[i-1] * layers[i]\n",
    "    weights[\"deep_bias_%d\" % i] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(1, self.deep_layers[i])),\n",
    "                dtype=np.float32)  # 1 * layer[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#到此为止的模型和DeepFM不能说一模一样，但确实毫无不同...\n",
    "#cross部分的参数 这里搭建了3层 根据论文的公式，每层cross layer的输出输入都是一样的\n",
    "for i in range(self.cross_layer_num):\n",
    "    weights[\"cross_layer_%d\" % i] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(self.total_size,1)),dtype=np.float32)\n",
    "    weights[\"cross_bias_%d\" % i] = tf.Variable(np.random.normal(loc=0, scale=glorot, size=(self.total_size,1)),dtype=np.float32)  # 1 * layer[i]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# final concat projection layer\n",
    "# 最后全连接层的输入为 self.total_size + self.deep_layers[-1] 看过上一篇文章你应该知道这是为什么了\n",
    "input_size = self.total_size + self.deep_layers[-1]\n",
    "glorot = np.sqrt(2.0/(input_size + 1))\n",
    "weights['concat_projection'] = tf.Variable(np.random.normal(loc=0,scale=glorot,size=(input_size,1)),dtype=np.float32)\n",
    "weights['concat_bias'] = tf.Variable(tf.constant(0.01),dtype=np.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 开始搭建模型\n",
    "## 首先还是熟悉的embedding_lookup  同样通过相乘将特征值为0的embedding删除。\n",
    "self.embeddings = tf.nn.embedding_lookup(self.weights['feature_embeddings'],self.feat_index) # [B,30,8]\n",
    "feat_value = tf.reshape(self.feat_value,shape=[-1,self.field_size,1]) #[B,30,1]\n",
    "self.embeddings = tf.multiply(self.embeddings,feat_value) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 构建输入 将【B,9】的离散特征和【B,240】的连续特征进行拼接\n",
    "\n",
    "self.x0 = tf.concat([self.numeric_value,tf.reshape(self.embeddings,shape=[-1,self.field_size * self.embedding_size])],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#送入deep之前经过了一个dropout\n",
    "self.y_deep = tf.nn.dropout(self.x0,self.dropout_keep_deep[0])\n",
    "for i in range(0,len(self.deep_layers)):\n",
    "    self.y_deep = tf.add(tf.matmul(self.y_deep,self.weights[\"deep_layer_%d\" %i]), self.weights[\"deep_bias_%d\"%i])\n",
    "    self.y_deep = self.deep_layers_activation(self.y_deep)\n",
    "    self.y_deep = tf.nn.dropout(self.y_deep,self.dropout_keep_deep[i+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# cross_part\n",
    "# 这部分就是按照论文中cross部分的公式，先保留一份_x0用于后续的残差\n",
    "\n",
    "# tf.matmul 矩阵相乘用transpose_b来进行转置 得到【B，249,249】 之后由于维度不相同，使用tf.tensordot乘以【249,1】\n",
    "# 【249,249】*【249,1】 = 【249,1】\n",
    "# 【249,1】 +【249，1】+【249,1】\n",
    "self._x0 = tf.reshape(self.x0, (-1, self.total_size, 1)) #【B,249,1】\n",
    "x_l = self._x0\n",
    "for l in range(self.cross_layer_num):\n",
    "    x_l = tf.tensordot(tf.matmul(self._x0, x_l, transpose_b=True) # 【B，249,249】,\n",
    "                       self.weights[\"cross_layer_%d\" % l],1) + self.weights[\"cross_bias_%d\" % l] + x_l\n",
    "    self.cross_network_out = tf.reshape(x_l, (-1, self.total_size))#【B,249】"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # concat_part \n",
    "\n",
    "concat_input = tf.concat([self.cross_network_out, self.y_deep], axis=1) #[B,249+32]\n",
    "self.out = tf.add(tf.matmul(concat_input,self.weights['concat_projection']),self.weights['concat_bias'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 模型到此就搭建完成了，之后还有一些loss选择和优化器选择的代码以及分batch训练的代码。这里就不细讲了，感兴趣的同学可以去阅读源码~"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#回到我们的main.py 得到训练以及测试数据后，调用模型的fit函数。"
   ]
  },
  {
   "attachments": {
    "1.png": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\n",
    "fit 函数首先会对输入的数据进行batch划分，这里就不详细介绍划分的过程了，代码也比较简单，得到的每个batch的数据如下，其中每条数据的特征维度分别为[30,30,9]\n",
    "\"\"\"\n",
    "![1.png](attachment:1.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_base_model_dcn(dfTrain, dfTest, folds, dcn_params):\n",
    "\n",
    "    fd = FeatureDictionary(dfTrain,dfTest,numeric_cols=config.NUMERIC_COLS,\n",
    "                           ignore_cols=config.IGNORE_COLS,\n",
    "                           cate_cols = config.CATEGORICAL_COLS)\n",
    "\n",
    "    print(fd.feat_dim)\n",
    "    print(fd.feat_dict)\n",
    "\n",
    "    data_parser = DataParser(feat_dict=fd)\n",
    "    cate_Xi_train, cate_Xv_train, numeric_Xv_train,y_train = data_parser.parse(df=dfTrain, has_label=True)\n",
    "    cate_Xi_test, cate_Xv_test, numeric_Xv_test,ids_test = data_parser.parse(df=dfTest)\n",
    "\n",
    "    dcn_params[\"cate_feature_size\"] = fd.feat_dim\n",
    "    dcn_params[\"field_size\"] = len(cate_Xi_train[0])\n",
    "    dcn_params['numeric_feature_size'] = len(config.NUMERIC_COLS)\n",
    "\n",
    "    _get = lambda x, l: [x[i] for i in l]\n",
    "\n",
    "    for i, (train_idx, valid_idx) in enumerate(folds):\n",
    "        cate_Xi_train_, cate_Xv_train_, numeric_Xv_train_,y_train_ = _get(cate_Xi_train, train_idx), _get(cate_Xv_train, train_idx),_get(numeric_Xv_train, train_idx), _get(y_train, train_idx)\n",
    "        cate_Xi_valid_, cate_Xv_valid_, numeric_Xv_valid_,y_valid_ = _get(cate_Xi_train, valid_idx), _get(cate_Xv_train, valid_idx),_get(numeric_Xv_train, valid_idx), _get(y_train, valid_idx)\n",
    "\n",
    "        dcn =  DCN(**dcn_params)\n",
    "\n",
    "        dcn.fit(cate_Xi_train_, cate_Xv_train_, numeric_Xv_train_,y_train_, cate_Xi_valid_, cate_Xv_valid_, numeric_Xv_valid_,y_valid_)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "pycharm": {
   "stem_cell": {
    "cell_type": "raw",
    "metadata": {
     "collapsed": false
    },
    "source": []
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
